<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.7//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_7.dtd">
<?xml-stylesheet type="text/css" href="http://oli.web.cmu.edu/authoring/oxy-author/oli_workbook_page_3_7.css"?>
<workbook_page xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:theme="http://oli.web.cmu.edu/presentation/" id="_u4_summary1">
<head>
<title>Summary (Probability)</title>
</head>
<body>
<p>This summary provides a quick recap of the material you've learned in the probability unit.
      Please note that this summary does not provide complete coverage of the material, but just
      lists the main points. We therefore recommend that you use this summary only as a checklist or
      a review before going on to the next unit, or before an exam. </p>
  <section><title>General Remarks</title>
  <body>
    <ul style="disc">
<li><p> Probability is a discipline by itself. In the context of the big picture of this course, probability is used to quantify the imperfection 
  associated with drawing conclusions about the entire population based only on a random sample drawn from it.</p></li>
<li><p>We talk about the probability of an event, which is a statement about the outcome of a random experiment. 
  In practice, each event corresponds to a subset of outcomes from the sample space.</p></li>
<li><p> The probability of an event can be as low as 0 (when the event is impossible) and as high as 1
              (when the event is certain).</p></li>
<li><p> In some cases, the only way to find the probability of an event of interest is by repeating the
              random experiment many times and using the relative frequency approach.</p></li>
<li><p> When all the possible outcomes of a random experiment are equally likely, the probability of an
              event is the fraction of outcomes which satisfy it.</p></li>
    </ul></body></section>
  <section>
    <title>Probability Principles</title>
    <body>
  <p> Probability principles help us find the probability of events of certain types:</p>
      <ul style="disc">
        <li><p><em>The Complement Rule</em>, P(not A) = 1 - P(A), is especially useful for finding events of the
              type "at least one of ..."</p></li>
<li><p>To find the probability of <em>events of the type "A or B" </em>(interpreted as A occurs or B occurs or both), we use the General Addition  Rule:
P(A or B) = P(A) + P(B) - P(A and B).</p>
  <p> In the special case when A and B are disjoint (cannot happen together; P(A and B) = 0) the
              Addition Rule reduces to: P(A and B) = P(A) + P(B).</p></li>
<li><p>To find the probability of <em>events of the type "A and B"</em> (interpreted as both A and B occur), we use the General Multiplication Rule:
P(A and B) = P(A) * P(B | A). </p>
  <p>In the special case when A and B are independent (the occurrence of one event has no effect on
              the probability of the other occurring; P(B | A) = P(B)) the Multiplication Rule
              reduces to: P(A and B) = P(A) * P(B).</p></li>
<li><p> P(B | A), the conditional probability of event B occurring given that event A has occurred, can
              be viewed as a reduction of the sample space S to event A. The conditional
              probability, then, is the fraction of event A where B occurs as well, P(B | A) = P(A
              and B) / P(A).</p></li>
      </ul>
<p>Probability trees are a useful visual tool for displaying and manipulating probabilities of events which naturally happen in sequence (or in
stages). It is particularly useful when we are given conditional probability in one direction  P(B | A), and need to find the 
reverse conditional probability P(A | B).</p>
    </body></section>
      
  <section>
        <title>Random Variables</title>
        <body>
     
      <p>A random variable is a variable whose values are numerical results of a random experiment.</p>
         <ul style="disc">
<li><p> A <em>discrete random variable</em> is summarized by its probability distribution&#8212;a list
              of its possible values and their corresponding probabilities. </p>
        <ul style="circle">
<li><p>The sum of the probabilities of all possible values must be 1.</p></li>
<li><p>The probability distribution can be represented by a table, histogram, or formula.</p></li>
        </ul>
</li>
        
<li><p> The <em>probability distribution</em> of a random variable can be supplemented with numerical
              measures of the center and spread of the random variable.</p>
  <ul style="circle">
<li><p><em>Center:</em> The center of a random variable is measured by its mean (which is sometimes also referred to as the <em>expected value</em>). </p>
<p> &#8212;The mean of a random variable can be interpreted as its long run average.</p>
<p> &#8212;The mean is a weighted average of the possible values of the random variable weighted by
                  their corresponding probabilities. </p>
<p> &#8212;An application of the mean of a random variable is determining premiums for insurance
                  policies.</p></li>
<li><p><em>Spread:</em> The spread of a random variable is measured by its variance, or more typically by its standard deviation  (the square root of the variance). </p>
<p> &#8212;The standard deviation of a random variable can be interpreted as the typical (or
                  long-run average) distance between the value that the random variable assumes and
                  the mean of X.</p></li>
    </ul></li>
<li><p>
              <em>Rules of means and variances</em> give us an easy way to find the mean and
              standard deviations of the "new" random variable a + bX (given the mean and standard
              deviation of X), as well as the mean and standard deviation of the "new" random
              variable X + Y (given the means and standard deviations of X and Y, and assuming that
              X and Y are independent).</p></li>
  
      </ul></body></section>
  <section>
    <title>Binomial Random Variables</title>
    <body>
  <ul style="disc">      
      <li><p> The binomial random variable is a type of discrete random variable that is quite common.</p></li>
<li><p>The binomial random variable is defined in a random experiment that consists of n independent
              trials, each having two possible outcomes (called "success" and "failure"), and each
              having the same probability of success: p. Such a random experiment is called the
              binomial random experiment.</p></li>
<li><p>The binomial random variable represents the number of successes (out of n) in a binomial
              experiment. It can therefore have values as low as 0 (if none of the n trials was a
              success) and as high as n (if all n trials were successes).</p></li>
<li><p>There are "many" binomial random variables, depending on the number of trials (n) and the
              probability of success (p).</p></li>
<li><p>The probability distribution of the binomial random variable is given in the form of a formula
              and can be used to find probabilities. Technology can be used as well.</p></li>
<li><p>The mean and standard deviation of a binomial random variable can be easily found using short-cut formulas.</p></li>
  </ul></body></section>
  
  <section>
    <title>Continuous Random Variables</title>
    <body>
 
  <p>The probability distribution of a continuous random variable is represented by a probability density curve. The probability that the
random variable takes a value in any interval of interest is the area above this interval and below the density curve.</p>
  
  <p>An important example of a continuous random variable is the <em>normal random variable</em>, whose probability density curve is 
  symmetric (bell-shaped), bulging in the middle and tapering at the ends.</p>
      <ul style="disc">
<li><p>There are "many" normal random variables, each determined by its mean <m:math overflow="scroll">
                <m:mi>μ</m:mi>
              </m:math> (which determines where the density curve is centered) and standard
              deviation <m:math overflow="scroll">
                <m:mi>σ</m:mi>
              </m:math>, (which determines how spread out (wide) the normal density curve is).</p></li>
  
<li><p> Any normal random variable follows the Standard Deviation Rule, which can help us find
              probabilities associated with the normal random variable.</p></li>
<li><p>Another way to find probabilities associated with the normal random variable is using the
              standard normal table. This process involves finding the z-score of values, which
              tells us how many standard deviations below or above the mean the value is.</p></li>
<li><p>An important application of the normal random variable is that it can be used as an approximation
              of the binomial random variable (under certain conditions). A continuity correction
              can improve this approximation.</p></li>
   </ul>
  </body></section>
  
  <section>
    <title>Sampling Distributions</title>
    <body>
     <p> A <em>parameter</em> is a number that describes the population, and a <em>statistic</em> is a number that describes the sample.</p> 
      <ul style="disc">
<li><p>Parameters are fixed, and in practice, usually unknown.</p></li>

<li><p>Statistics change from sample to sample due to sampling variability.</p></li>
<li><p>The behavior of the possible values the statistic can take in repeated samples is called the <em>sampling distribution</em> of that statistic.</p></li>
 </ul>
  
  
  <p>The <em>sampling distribution of the sample proportion</em>, <m:math overflow="scroll">
            <m:mover accent="true">
              <m:mrow>
                <m:mi>p</m:mi>
              </m:mrow>
              <m:mo>ˆ</m:mo>
            </m:mover>
          </m:math>, (under certain conditions): </p>
      <ul style="disc">
<li><p>is centered around p, the proportion in the entire population from which the sample is drawn.</p></li>

<li><p>has standard deviation of <m:math overflow="scroll">
                <m:msqrt>
                  <m:mrow>
                    <m:mfrac>
                      <m:mrow>
                        <m:mi>p</m:mi>
                        <m:mo>(</m:mo>
                        <m:mn>1</m:mn>
                        <m:mo>-</m:mo>
                        <m:mi>p</m:mi>
                        <m:mo>)</m:mo>
                      </m:mrow>
                      <m:mrow>
                        <m:mi>n</m:mi>
                      </m:mrow>
                    </m:mfrac>
                  </m:mrow>
                </m:msqrt>
              </m:math>. </p></li>
<li><p>is approximately normal (under certain conditions).</p></li>
        </ul>
<p>According to the <em>Central Limit Theorem</em>, the <em>sampling distribution of the sample
            mean</em>, <m:math overflow="scroll">
            <m:mover accent="true">
              <m:mrow>
                <m:mi>X</m:mi>
              </m:mrow>
              <m:mo>¯</m:mo>
            </m:mover>
          </m:math>:</p>
        <ul style="disc">
<li><p>is centered around <m:math overflow="scroll">
                <m:mi>μ</m:mi>
              </m:math>, the mean in the entire population from which the sample is drawn. </p></li>
<li><p>has a standard deviation of <m:math overflow="scroll">
                <m:mfrac>
                  <m:mrow>
                    <m:mi>σ</m:mi>
                  </m:mrow>
                  <m:mrow>
                    <m:msqrt>
                      <m:mrow>
                        <m:mi>n</m:mi>
                      </m:mrow>
                    </m:msqrt>
                  </m:mrow>
                </m:mfrac>
              </m:math>. </p></li>
<li><p>is, for a large enough sample size n, approximately normal (regardless of the shape of the
              population distribution).</p></li>
  </ul>
      </body></section>
</body>
</workbook_page>
