<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.7//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_7.dtd">
<?xml-stylesheet type="text/css" href="http://oli.web.cmu.edu/authoring/oxy-author/oli_workbook_page_3_7.css"?>
<workbook_page xmlns:pref="http://oli.web.cmu.edu/preferences/"
    xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:theme="http://oli.web.cmu.edu/presentation/"
    id="_u5_m1_wonder2">
    <head>
        <title>Hypothesis Testing for the Population Proportion</title>
    </head>
    <body>
        <p> Many students wonder why 5% is often selected as the significance level in hypothesis
            testing, and why 1% is the next most typical level. This is largely due to just
            convenience and tradition. </p>
        
        <p>When Ronald Fisher (one of the founders of modern statistics) published one of his
            tables, he used a mathematically convenient scale
            <?oxy_insert_start author="mmyers" timestamp="20101127T131509-0500"?>that<?oxy_insert_end?><?oxy_delete author="mmyers" timestamp="20101127T131507-0500" content="which"?>
            included 5% and 1%. Later, these same 5% and 1% levels were used by other people, in
            part just because Fisher was so highly esteemed. But mostly these are arbitrary levels. </p>
        
        <p>The idea of selecting some sort of relatively small cutoff was historically important in
            the development of statistics; but it’s important to remember that there is really a
            continuous range of increasing confidence towards the alternative hypothesis, not a
            single
            all-or-nothing<?oxy_insert_start author="mmyers" timestamp="20101127T131539-0500"?>
            value<?oxy_insert_end?>. There isn’t much meaningful difference, for instance, between a
            p-value of .049 or .051, and it would be foolish to declare one case definitely a
            <?oxy_insert_start author="mmyers" timestamp="20101127T131552-0500"?>"<?oxy_insert_end?><?oxy_delete author="mmyers" timestamp="20101127T131552-0500" content="‘"?>real<?oxy_insert_start author="mmyers" timestamp="20101127T131554-0500"?>"<?oxy_insert_end?><?oxy_delete author="mmyers" timestamp="20101127T131554-0500" content="’"?>
            effect and to declare the other case definitely a
            <?oxy_insert_start author="mmyers" timestamp="20101127T131558-0500"?>"<?oxy_insert_end?><?oxy_delete author="mmyers" timestamp="20101127T131558-0500" content="‘"?>random<?oxy_insert_start author="mmyers" timestamp="20101127T131601-0500"?>"<?oxy_insert_end?><?oxy_delete author="mmyers" timestamp="20101127T131601-0500" content="’"?>
            effect. In either case, the study results were roughly 5% likely by chance if there’s no
            actual effect. </p>
        
        <p>Whether such
            a p-value is sufficient for us to reject a particular null hypothesis ultimately depends
            on the risk of making the wrong decision, and the extent to which the hypothesized
            effect might contradict our prior experience or previous studies. </p>
    </body>
</workbook_page>
